-Previous session we discussed about Person, Contract and Product
-We're clear about hierarchy Person > Contract > Product
-We saw how data is in Sync,
	-If person detail is available in Person table, then in contract table we have subscription details against the same person and it can be identified by MDMID
	-Person details of an individual can be found in Person table and contract related details in the contract table
	-Products related to person in the product table can be found using the contract ID or Person MDMID
	
Note: For all the Dags of Person, Contract and Product, the main data is from MDM side and JPD acts as lookup for it

Count check added in the Dags (Contract, Product & Person) : It will check whether there are atleast 10 new records on daily basis, if not it will shoot an email stating count is less than 10 for a product, person and contract for the given day

-Person Dag (mdm-person-to-pega):
	-Main Dag is mdm-person-to-pega.py
		-Inside this we're calling multiple other modules
			1. person_lookup_datasets.py
				-We generate below data:
					-fetch_person_invalid_emails
					-generate_pega_bounced_emails
					-generate_pega_mdm_elisaperson
					-generate_postiviidakko_bounced_emails
				-Ex of Business use case, PEGA wanted to know history emails got bounced in last 6 months, we didn't have those details in the person table, we do have it in other tables called as Postiviidakko and also some data in email history table
				-PEGA wanted Data Engineer team to join those two table with Person table to generate a column to show the bounced code
					-Hard Bounce : Email ID is incorrect
					-Soft Bounce : Errors during email is sent, server break down etc
				-Main Intent was to know issue is from customer side like customer has deliberately provided incorrect details or something at Elisa end
			2. person_pega_mdm_elisaperson.py
			3. person_to_datalake.py
			

-There are three tables in total used for marketting condition:
	1. EMAIL_HISTORY_QV
	2. Postiviidakko_CONDITIONAL_CUSTOMER
	3. Postiviidakko_CONDITIONAL_KIRJA
-A view is created by doing a union to create a view to know how many bounced from PEGA side, KIRJA side etc

Email History:
--------------
-The main table is Email_History and it does have sub tables like EMAIL_HISTORY_qv as well
-The data in EMAIL_HISTORY_qv is not generated by Data Engineer team, instead it is directly coming from PEGA, PEGA generates emails and sends to customer, all those related details come to this table, but EMAIL_HISTORY_qv table will not contain all the emails from Elisa
-Schema will be like,
	-Offer Name
	-Offer Group
	-OPENED
	-URL
	:
	:
	:
*************Note: It's not like only PEGA sends email to customers, outside PEGA there are other systems as well which sends email to customer and one of other system is called as "Postiviidakko" (It's a system name, no proper translation), this data is also used by Elisa to send emails to customer
	-Schema for Postiviidakko goes like this:
		1. EMAIL
		2. SMS
		3. ETUNMI (First Name)
		4. MDMID
		5. Delivery (Helps in uniquely identifying record based on email id)
		6. OPENER_DATE ##Generated by Data Engineers, understand the logic
		7. OPENER_NUMBER ##Generated by Data Engineers, understand the logic
		8. BOUNCED ##Generated by Data Engineers, understand the logic
		9. DELIVERED ##Generated by Data Engineers, understand the logic
		10. ACCOUNT_ID : Elisa messaging account IDs (Ex: 13947)
		11. Clicked
		:
		:
		:

-Points to remember:
	1. RTMP-DATA_SCIENTISTS : Production Server
	2. RTMP-DATA_SCIENTISTS : Test Server
	3. KIRJA (Pronounced : Keerya, means e-books) : (Ex: 8865), in a separate tables (not in postiidakko) details around communication w.r.t. e-books is stored here
	
-In Person data table above table datas are used i.e. from Postiviidakko, EMAIL_HISTORY_qv and KIRJA to generate Bounce CODE ( 1 is soft code and 2 is Hard Code)
-Mainly PEGA is interested in Hard Code

-----------

-Then comes the processing tasks, module used here is person_pega_mdm_elisaperson.py
	1. For Processing person data it requires additional data such as vcompany, vdecisionmakerrole etc, basically corporate related data
	2. Above tables are also looked_up
	3. After above tables are looked_up through joining, finally output is joined with person table only i.e. MDM_ELISAPERSONFORPEGA
	4. Post all processing, grouping and filtering an ouput by name fl_elisaperson (check it once) is generated and loaded into Data Lake
	
-Then the person_to_datalake.py, reads data from previous tasks, reads it, performs some more processing & filtering and finally performs the incremental logic and loads data to Data Lake

-In this Person we're generating 4 tables (a copy is available in on prem DB),
	1. fl_person : Will be soon stopped loading to on prem, since nobody using it, will be retained only in Data Lake
	2. person : Main data available here, limited number of columns used for marketting, Data Engineers use this most
	3. person_ext : Main data available here as well, additional columns are there, used by target group planners etc
	4. person_new : Main data available here as well, dedicated to PEGA, in total there are 190 columns, ******only this data is stored in Data Lake Delta location
		aws s3 ls s3://baikal-pega/parquet/person/person_new/
		delta/ ##Incremental data sent from Kafka Utils to Kafka Topics, from which PEGA reads the data
		final/

Note: Any Retention Period?
	Data Lake:
		1. Think about, on daily basis we're loading full data to Data Lake
		2. The data created 1 year back will be removed, worst case also we still be having the full data, as it is loaded on daily basis
	On Prem DB:
		1. Retention of Two years is set-up
		2. Using Stored Procedures it is deleted based on set retention policy

******Note:
1. biakal-pega bucket is only for our purpose, Ravi and Yogesh has access to write the data
2. To view vertica, MDM Data / Raw data, jpd on the Data Lake, we need to use baikal-snapshot
	aws s3 ls s3://baikal-snapshot/parquet
		vertica.SVOC_BUS.ST_MOBCOM_WP10_APARTMENTDATA/
		:
		:
		:
		viihde.....
		:
		:
		mdm.integration.ElisaPersonContractForPega/
		mdm.integration.ElisaPersonFOrPega/
		mdm.integration.ElisaPersonMarketingPermissionForPega/
		:
		:	
		jpd.dbserver.sacc.vds12/
		:
		:
		:
		
******vertica.SVOC_BUS.ST_MOBCOM_WP10_APARTMENTDATA/
vertica : Depicts Server details
SVOC_BUS : Schema name
Table_Name : ST_MOBCOM_WP10_APARTMENTDATA
Default Topic Name for fututre purpose is complete name : "vertica.SVOC_BUS.ST_MOBCOM_WP10_APARTMENTDATA"

|||rly in MDM below are two main source data for Person, Product and COntract:
mdm.integration.ElisaPersonContractForPega/
mdm.integration.ElisaPersonFOrPega/
ElisaPersonContractForPega : It's a View in the MDM system (SQL Server System) created on few tables, which will provide Contract + Person details

*******jpd data, coming from debezium connector and these are real-time in nature, but in data lake loaded once in a Day, how it is loaded to Data Lake???
jpd data > debezium push to Kafka Topic > There is Kafka Connector which loads data from Kafka Topic to Data Lake in some another bucket (baikal-kafka) > from there, there is a separate script which picks those data from bucket, checks those incremental and other things and altogether create one snapshot that is historical data and finally load to baikal-snapshot bucket everyday
---------------------------------------------------------------------------------

Postiviidakko:
==============
-There is a common Dag for Postiviidakko and Viestipalvelu named as "fetch-datalake-to-pega-rtm"
-This Dag helps in pushing data available on the Data Lake to OnPrem or to PEGA for any reason, we can use this Dag and add new modules for example an reuirement to push data to new DB, just add a module and push data to it, bas that's it
-The new module to be added under data sources and called as a task in the "fetch-datalake-to-pega-rtm" Dag
-*********There are two types of data available on Data Lake which is very important for marketting purpose they're,
	1. Postiviidakko : Data sent via mail
	2. Viestipalvelu : 
		-Customer SMS sent, sent via Viestipalvelu API, includes all the data from PEGA related and Elisa messaging account as well
		-Currently Yogesh is working on the same code to add the PEGA account 
		-Currently there is a script that fetches Elisa messaging account details from Viestipalvelu, it doesn't have PEGA account details, Yogesh will be shortly joining them

--------------
-The Dag for Viestipalvelu is very simple,
-To view the Viestipalvelu data on Data Lake,
	aws s3 ls s3://baikal-snapshot/parquet/viestipalvelu/
		##It has two types of data, latest and date wise folder
		2023/
		latest/
	aws s3 ls s3://baikal-snapshot/parquet/viestipalvelu/2023/04
		02/
		03/
		04/
		05/
		06/
		11/
		12/
-As per the Dag, only if the latest data available for Viestipalvelu check is +ve, only then it will process the data (addition of few columns LOAD_Date, convert timestamp for "created" & "statustime" to EET) and next complete
-Later Truncate and load to On Prem

--------
-Loading of Postiviidakko data for Dag with initial check whether the latest data is available, if yes, chalo kaam shuru karo,
-Here the data structure is impler, but storing structure is complex
-Below is the path for the Postiviidakko data on the Data lake,
	aws s3 ls s3://baikal-snapshot/parquet/postiviidakko/
		2023-04-01/
		2023-04-02/
		:
		:
		:
	aws s3 ls s3://baikal-snapshot/parquet/postiviidakko/2023-04-12/
		13947/
		13948/
		8008/
		8865/
		9675/
	##For us, only two accounts are important,
		##13947 : Used for Elisa Messaging
		##8865 : Used for Kirja (pronounced as keerya, for e-books)
	aws s3 ls s3://baikal-snapshot/parquet/postiviidakko/2023-04-12/13947/
		delivery-members-conditional-all/
		delivery-members-conditional-bounced/
		delivery-members-conditional-delivered/
		delivery-members-conditional-openers/
		delivery-members-conditional-unsubscribed/
		mail-link-data/
		mail-link-tracking-data/ ##Not used by Data Engineers
		mailing-list-members-conditional/
		tracling-open-info/ ##Not used by Data Engineers
		deliveries.json 
			##There is one section, were we directly load this deliveries.json data directly, has consolidated data and has delivery id mapped for a mail id 
			##HY_DATA_SCIENTISTS.POSTIVIIDAKKO_DELIVERY has the delivery ID and other details and which can be joined with ##HY_DATA_SCIENTISTS.POSTIVIIDAKKO_CONDITIONAL_CUSTOMER table for better insights
	aws s3 ls s3://baikal-snapshot/parquet/postiviidakko/2023-04-12/13947/delivery-members-conditional-all/
			delivery_id=5343854/
			delivery_id=5347648/
			delivery_id=5356048/
******Note: 
-The folder structure is followed in this way is to meet the structure similar to of how the data is stored in the Postiviidakko API, so there is a script which connects to Postiviidakko API fetches all the data
-The delivery-members-conditional-all/ ideally means has consolidated data for bounced, delivered, openers and unsubscribed, but at times all/ will not have the updated data of bounced, delivered, openers and unsubscribed
-To overcome above issue, we need to trace back all the folders or else we create all/ has main source data and do a left join with bounced/, delivered/, openers/ and unsubscribed/ and a new column is populated in all/ like bounced, delivered, openers and unsubscribed with 0 or 1 value

	aws s3 ls s3://baikal-snapshot/parquet/postiviidakko/2023-04-12/13947/delivery-members-conditional-all/delivery_id=5343854/
			data.csv ##The actual readable data is present over here
	aws s3 ls s3://baikal-snapshot/parquet/postiviidakko/2023-04-12/13947/mail-link-data/delivery_id=5343854/
			data.json ##The actual readable data is present over here
--------------------

postiviidakko_file_tracker task:
	-This is a little complex thing
	-We're passing start and end date for this, like for how many days you want to process this data, because if it is for the first time, then initial dates are required, we would need to load 1 year data
	-If it is not for the 1st time, then it is normal contracts date can be passed, then T-1 data will be read
	-To process the data, 1st a list is created, list is created for 13947 and 8865
	-Inside the list of 13947 a list is created for all/ and inside all/ we have the deliuvery_id
	-Therefore a list of path is created for ex:
		1st > s3://baikal-snapshot/parquet/postiviidakko/2023-04-12/
		2nd > " " + delivery-members-conditional-all/
		3rd > " " + delivery_id=5343854/
	-Then it will repeat for remaining two delivery id's
	-Next it goes on for bounced/, delivered/, openers/,  unsubscribed/ and mail-link-data/
******Note: List way of running the code is better approach, instead of creating spark sessions again and again
	-Once all the above tasks are completed, it will save it in the XCOM push and XCOM push output is utilized in the Spark job that's cool, this is better or optimized way of first creating list, after all the data is read and XCOM push is done, at last Spark job (instead of running multiple spark sesssions in loop) is started and directly uses the consolidated data output from XCOM
	-Once the data load whether the initial load / T-1 load is finalised
	-Next is the Python functions for get list, after that update list, acoount list
	-Finally we get below three list for,
		1. Conditional
			-The schema was not matching, to keep code generic, schema is made common across all the conditional types
			-Loaded to prev folder
			-Getting the latest data from 7 days data and loading to final folder
			-In the output file generated, KIRJA there is 48 columns and in conditional data there is 22 columns
			- Our nextr task is to add columns in conditional and columns are bounced, delivered, openers and mailing without affecting other columns for that generate_sql function is used
			-Adding tilt '`' to Scavadian characters to enable Spark to read it
		2. Kirja
			-Even for Kirja, similar logic is followed
		For Point 1 and 2 after all required transformations, data is loaded to Final_Target table, from this location only data is picked and loaded to on-prem DB
		3. delivery
			-At times creating struct_type is required i.e. schema
			-Decrypt of data like email subject and email body is required for delivery list
			-Later transformation of the data, related to change of data type etc
		Above three lists are pushed by XCOM
	-Once lists are ready we call three functions (includes Spark) for all the above three lists
	-Data is loaded Datalake first
		-Initially it is loaded to prev folder
		-Then a grouping together is performed on the latest date
********-In Postiviidakko we have one more condition, consider today the event is generated and an email is sent, postiviidakko will track that data for next 7 days, to undertsand how many times it is opened and clicks are there, so for next 7 days we keep getting updates and it will be incremental, therefore last date is important to us, following condition is applied to generate a row_number
			PARTITION BY USER_ID, ID ORDER BY LOAD_DATE DESC
	-Then loaded to final/ folder and for synching purpose, from final/ it is loaded to prev/, which also acts as back up
	-Post which loaded to on prem DB
	-In the DAG we also have task for house keeping, which takes care of data retention policy and holds data for last 1 year, anything beyond is removed, but house keeping not applicable for delivery as it has 2 years of retention, taken care separately
	
Note: We also have MAIL_LINK data, which has only 4 columns, we need to do group by EMAIL, DELIVERY_ID to get a count of how many clicked has happened

Future:
-If there are any new schemas, add the account id, if it comes under new account, so it can be called
-Schema needs to be considered


*******************Note: The code for postiviidakko.py (haveing entire thing explained above) and viestipalvelu.py is created as modules under airflow-jobs/dags/pega/data_sources/ and called whenever it is required