Start Looking from Line # 191
==================================================================
-Previous session we discussed about Person, Contract and Product
-We're clear about hierarchy Person > Contract > Product
-We saw how data is in Sync,
	-If person detail is available in Person table, then in contract table we have subscription details against the same person and it can be identified by MDMID
	-Person details of an individual can be found in Person table and contract related details in the contract table
	-Products related to person in the product table can be found using the contract ID or Person MDMID
	
Note: For all the Dags of Person, Contract and Product, the main data is from MDM side and JPD acts as lookup for it

Count check added in the Dags (Contract, Product & Person) : It will check whether there are atleast 10 new records on daily basis, if not it will shoot an email stating count is less than 10 for a product, person and contract for the given day

-Person Dag (mdm-person-to-pega):
	-Main Dag is mdm-person-to-pega.py
		-Inside this we're calling multiple other modules
			1. person_lookup_datasets.py
				-We generate below data:
					-fetch_person_invalid_emails
					-generate_pega_bounced_emails
					-generate_pega_mdm_elisaperson
					-generate_postiviidakko_bounced_emails
				-Ex of Business use case, PEGA wanted to know history emails got bounced in last 6 months, we didn't have those details in the person table, we do have it in other tables called as Postiviidakko and also some data in email history table
				-PEGA wanted Data Engineer team to join those two table with Person table to generate a column to show the bounced code
					-Hard Bounce : Email ID is incorrect
					-Soft Bounce : Errors during email is sent, server break down etc
				-Main Intent was to know issue is from customer side like customer has deliberately provided incorrect details or something at Elisa end
			2. person_pega_mdm_elisaperson.py
			3. person_to_datalake.py
			

-There are three tables in total used for marketting condition:
	1. EMAIL_HISTORY_QV
	2. Postiviidakko_CONDITIONAL_CUSTOMER
	3. Postiviidakko_CONDITIONAL_KIRJA
-A view is created by doing a union to create a view to know how many bounced from PEGA side, KIRJA side etc

Email History:
--------------
-The main table is Email_History and it does have sub tables like EMAIL_HISTORY_qv as well
-The data in EMAIL_HISTORY_qv is not generated by Data Engineer team, instead it is directly coming from PEGA, PEGA generates emails and sends to customer, all those related details come to this table, but EMAIL_HISTORY_qv table will not contain all the emails from Elisa
-Schema will be like,
	-Offer Name
	-Offer Group
	-OPENED
	-URL
	:
	:
	:
*************Note: It's not like only PEGA sends email to customers, outside PEGA there are other systems as well which sends email to customer and one of other system is called as "Postiviidakko" (It's a system name, no proper translation), this data is also used by Elisa to send emails to customer
	-Schema for Postiviidakko goes like this:
		1. EMAIL
		2. SMS
		3. ETUNMI (First Name)
		4. MDMID
		5. Delivery (Helps in uniquely identifying record based on email id)
		6. OPENER_DATE ##Generated by Data Engineers, understand the logic
		7. OPENER_NUMBER ##Generated by Data Engineers, understand the logic
		8. BOUNCED ##Generated by Data Engineers, understand the logic
		9. DELIVERED ##Generated by Data Engineers, understand the logic
		10. ACCOUNT_ID : Elisa messaging account IDs (Ex: 13947)
		11. Clicked
		:
		:
		:

-Points to remember:
	1. RTMP-DATA_SCIENTISTS : Production Server
	2. RTMP-DATA_SCIENTISTS : Test Server
	3. KIRJA (Pronounced : Keerya, means e-books) : (Ex: 8865), in a separate tables (not in postiidakko) details around communication w.r.t. e-books is stored here
	
-In Person data table above table datas are used i.e. from Postiviidakko, EMAIL_HISTORY_qv and KIRJA to generate Bounce CODE ( 1 is soft code and 2 is Hard Code)
-Mainly PEGA is interested in Hard Code

-----------

-Then comes the processing tasks, module used here is person_pega_mdm_elisaperson.py
	1. For Processing person data it requires additional data such as vcompany, vdecisionmakerrole etc, basically corporate related data
	2. Above tables are also looked_up
	3. After above tables are looked_up through joining, finally output is joined with person table only i.e. MDM_ELISAPERSONFORPEGA
	4. Post all processing, grouping and filtering an ouput by name fl_elisaperson (check it once) is generated and loaded into Data Lake
	
-Then the person_to_datalake.py, reads data from previous tasks, reads it, performs some more processing & filtering and finally performs the incremental logic and loads data to Data Lake

-In this Person we're generating 4 tables (a copy is available in on prem DB),
	1. fl_person : Will be soon stopped loading to on prem, since nobody using it, will be retained only in Data Lake
	2. person : Main data available here, limited number of columns used for marketting, Data Engineers use this most
	3. person_ext : Main data available here as well, additional columns are there, used by target group planners etc
	4. person_new : Main data available here as well, dedicated to PEGA, in total there are 190 columns, ******only this data is stored in Data Lake Delta location
		aws s3 ls s3://baikal-pega/parquet/person/person_new/
		delta/ ##Incremental data sent from Kafka Utils to Kafka Topics, from which PEGA reads the data
		final/

Note: Any Retention Period?
	Data Lake:
		1. Think about, on daily basis we're loading full data to Data Lake
		2. The data created 1 year back will be removed, worst case also we still be having the full data, as it is loaded on daily basis
	On Prem DB:
		1. Retention of Two years is set-up
		2. Using Stored Procedures it is deleted based on set retention policy

******Note:
1. biakal-pega bucket is only for our purpose, Ravi and Yogesh has access to write the data
2. To view vertica, MDM Data / Raw data, jpd on the Data Lake, we need to use baikal-snapshot
	aws s3 ls s3://baikal-snapshot/parquet
		vertica.SVOC_BUS.ST_MOBCOM_WP10_APARTMENTDATA/
		:
		:
		:
		viihde.....
		:
		:
		mdm.integration.ElisaPersonContractForPega/
		mdm.integration.ElisaPersonFOrPega/
		mdm.integration.ElisaPersonMarketingPermissionForPega/
		:
		:	
		jpd.dbserver.sacc.vds12/
		:
		:
		:
		
******vertica.SVOC_BUS.ST_MOBCOM_WP10_APARTMENTDATA/
vertica : Depicts Server details
SVOC_BUS : Schema name
Table_Name : ST_MOBCOM_WP10_APARTMENTDATA
Default Topic Name for fututre purpose is complete name : "vertica.SVOC_BUS.ST_MOBCOM_WP10_APARTMENTDATA"

|||rly in MDM below are two main source data for Person, Product and COntract:
mdm.integration.ElisaPersonContractForPega/
mdm.integration.ElisaPersonFOrPega/
ElisaPersonContractForPega : It's a View in the MDM system (SQL Server System) created on few tables, which will provide Contract + Person details

*******jpd data, coming from debezium connector and these are real-time in nature, but in data lake loaded once in a Day, how it is loaded to Data Lake???
jpd data > debezium push to Kafka Topic > There is Kafka Connector which loads data from Kafka Topic to Data Lake in some another bucket (baikal-kafka) > from there, there is a separate script which picks those data from bucket, checks those incremental and other things and altogether create one snapshot that is historical data and finally load to baikal-snapshot bucket everyday
---------------------------------------------------------------------------------

Postiviidakko:
==============
-There is a common Dag for Postiviidakko and Viestipalvelu named as "fetch-datalake-to-pega-rtm"
-This Dag helps in pushing data available on the Data Lake to OnPrem or to PEGA for any reason, we can use this Dag and add new modules for example an reuirement to push data to new DB, just add a module and push data to it, bas that's it
-The new module to be added under data sources and called as a task in the "fetch-datalake-to-pega-rtm" Dag
-*********There are two types of data available on Data Lake which is very important for marketting purpose they're,
	1. Postiviidakko : Data sent via mail
	2. Viestipalvelu : 
		-Customer SMS sent, sent via Viestipalvelu API, includes all the data from PEGA related and Elisa messaging account as well
		-Currently Yogesh is working on the same code to add the PEGA account 
		-Currently there is a script that fetches Elisa messaging account details from Viestipalvelu, it doesn't have PEGA account details, Yogesh will be shortly joining them

--------------
-The Dag for Viestipalvelu is very simple,
-To view the Viestipalvelu data on Data Lake,
	aws s3 ls s3://baikal-snapshot/parquet/viestipalvelu/
		##It has two types of data, latest and date wise folder
		2023/
		latest/
	aws s3 ls s3://baikal-snapshot/parquet/viestipalvelu/2023/04
		02/
		03/
		04/
		05/
		06/
		11/
		12/
-As per the Dag, only if the latest data available for Viestipalvelu check is +ve, only then it will process the data (addition of few columns LOAD_Date, convert timestamp for "created" & "statustime" to EET) and next complete
-Later Truncate and load to On Prem

--------
-Loading of Postiviidakko data for Dag with initial check whether the latest data is available, if yes, chalo kaam shuru karo,
-Here the data structure is impler, but storing structure is complex
-Below is the path for the Postiviidakko data on the Data lake,
	aws s3 ls s3://baikal-snapshot/parquet/postiviidakko/
		2023-04-01/
		2023-04-02/
		:
		:
		:
	aws s3 ls s3://baikal-snapshot/parquet/postiviidakko/2023-04-12/
		13947/
		13948/
		8008/
		8865/
		9675/
	##For us, only two accounts are important,
		##13947 : Used for Elisa Messaging
		##8865 : Used for Kirja (e-books)
	s3://baikal-snapshot/parquet/postiviidakko/2023-04-12/13947
	
=========================================================================	
		
Airflow Dag : jpd-product-to-pega:
----------------------------------

-Till now we understand that, main critical data for marketting purpose is Person, Contract and then Product and hierarchy is also the same order
-The Person, COntract and Product always needs to be in Sync
-Each person in Elisa has a unique ID called as Amper ID or MDM ID
-FOr PEGA marketting Person, Contract and Product needs to be in Sync and these three are main tables required for marketting purpose

Start-Process Task:
-------------------
-It creates a CSV file with the data in the Datalake in the path we've provided 
-File name will be DagName_ExecutionDate_StartTime

End-Process Task:
-----------------
-End the process, add the end_time and overwrite the file
-We can check in S3, whether file is created, from there we can easily identify whether task has ran successfully or not

find_latest_s3_data:
--------------------
-Going back multiple times and checking for latest file under the Date wise folder (it will start cchecking from T-1 days)
-Iteration which it performs can be controlled, we're currently using 8 times
-There is an utility called s3.exists, this helps in checking whether file exists inside the bucket
-This basically looks for file with '_SUCCESS'

*****Note:
-In "s3://baikal-pega/parquet/contract/final/"
baikal-pega/ > Is the bucket name
/parquet/contract/final/ > Is the key template


jpd_base_common_details:
------------------------
-The product process starts here
-The data comes from jpd
-First of all, here the scenario is related to the product table
-The jpd data is a intermediate data, not loaded to any table and basically located at Datalake,
	aws s3 ls s3://baikal-pega/parquet/product/
		jpd_base/
		##Under jpd_base/ we can find all the part files, this main jpd data coming from jpd and utilized in the product table
-We can check the product ID in this jpd base data
-Scenario: If customer asks, this product id is not available in product table, why??
	-First check whether this Product ID details is available at MDM side? (Check next sessions to know the table name)
	-If this product details is available in jpd side? (we can check the on Datalake path, to know the details)
	-If the data exists in that path, next go through further tasks to debug at which step it basically went missing?
		##i.e. look into the below tasks,
			-generate_jpd_base_batch1
			-generate_jpd_base_batch2
-As we're debugging we need to understand one more thing, jpd common, in jpd_common we have generated some common data which is being used everywhere, i.e. contract details
*******The contract details data is a join of contract, product and YHTEYSTIEDOT(pronounced sthetestode, which is customer contact details)
-If the product is not here in the contract details, it means that, it is not coming from the jpd side
-For more investigation, we can also look into the JPD_CONTRACT table, it's like a subscription data, coming from JPD side


generate_jpd_base_batch1:
-------------------------
-It is helped in joining the different datasets to main datasets, ex: handset data (fetched from hand-set, hand-model and device table) and finally joining with contract details, to get the other subscription related details for that particular product
-Finally we get our jpd_base
	-Get to know how account id is created etc
-In product dag main problem is around debugging...
-It's more of differentiating whether it belongs to Viihde / handset within product, the key to it is important

generate_jpd_base_batch2:
--------------------------
-Check for batch2 on 11th day video

generate_prodholding_batch1 & batch2:
------------------------------------
-Prod holding is in series with jpd_base as it consumes the data generated by jpd_base
-Here the condition is little bit different, though it is considering the jpd_base data, it isn't reading all of them, but considering few by applying where condition
-Prod holding is also helpful, in few cases such as a column is null in MDM base, we can use jpd_base or prod_holding to generate it, if it's available
-It joins the jpd_base with few other data and process it, prior to generating a valuable output
-Even Kirja data is also generated here

-After Batch 1 & 2 for both jpd_base & prod_holding, I'll be having two sets of data in hand, one for jpd base and other for prod_holding, can be viewed at the below path in Datalake,
	aws s3 ls s3://baikal-pega/parquet/product/
	-jpd_base
	-prodholding_base
	
wait_elisapersoncontract_data:
------------------------------
-This is a customized Sensor (custom operators are nothing but wrapper classes), called as S3PrefixSensor, it will wait until the path is available successfully
-Then it will tell, path is present, please proceed and path is related to mdm data i.e. mdm.integration.ElisaPersonContractForPega
-As known earlier, for Person, Contract and Product, main source of data is mdm, it's an enriched data, generated from jpd

generate_pega_mdm_product:
---------------------------
-This is the main data source
-This jpd_base and prod_holding acts as a lookup for the mdm data
-If the mdm data isn't available, we will let the Dag fail, because it is mdm team's headache to generate it

fl_pega_mdm_elisaproduct:
-------------------------
-Reads data from ELISA_PERSON_CONTRACT latest data
-Here the Handset, Device and ELISA_CONTRACT_DATA (this is main source data for everything) is required
-Here it is reading jpd_data and adding Person details to it from mdm
-This will finally create, fl_pega_mdm_elisaproduct/ in the below path,
	aws s3 ls s3://baikal-pega/parquet/product/

*********Note:
-MDM also gets data from JPD
-There is a slight difference in the calling terms for two columns,in jpd and pega
-In jpd whatever the data comes, the customer code in jpd is contract id for us and the contractindex in jpd is product id for us

check_new_snapshot_exists:
---------------------------
-The product which is final is generated in the final, generated by us
-Check is kept just because, an incremental is also performed, post which a success file is available in that final folder, it will tell the check process that, this process has already been ran yesterday and there is a success data in final folder and can be considered as previous day data and today's data I can take it as latest and compare to know which are deleted and which are inserted
-Inside the final/ in the path, 
		aws s3 ls s3://baikal-pega/parquet/product/
		final/
		##We have success file


fl_product_data:
----------------
-Additional filters are applied on the data of fl_pega_mdm_elisaproduct/
-Join with handset, device etc, process it
-After which it will create fl_product/ in the path,
		aws s3 ls s3://baikal-pega/parquet/product/
-Active, Future and Closed


generate_last_product_change_data:
----------------------------------
-This is nothing but joining contract and product table at jpd side, it's a self join to identify that, to understand which is previous product date, days, time like closed time etc


======================================================================================================================================================

Contract: (jpd-contract-to-pega.py):
------------------------------------
-Even here we're reading data from MDM, even here we're waiting for same dataset i.e. "wait_ElisaPersonContractForPega_data"
-Enriching the data, adding billing details on top of it, loading it to DB and sending to PEGA

Person: (mdm-person-to-pega):
------------------------------
-The source is again same MDM, the dataest for which we need to wait is for "wait_ElisaPersonForPega"
