Spark-SQL : 25Jan2022
-We still have a Spark-Core which consists of RDD.
-On top of the spark-core Spark-SQL is built.
-Unlike Scala PySpark only supports Dataframes and not the Datasets.
-When anything is submitted to Spark-SQL will get submitted to catalyst optimizer this inturns convert into RDDs, the catalyst takes care of creating logical and physical plan(RDD).
-Spark SQL > DataFrames > Catalyst Optimizer > RDDs > Partitions > Executors.
-Spark Session is an object which internally consists of two more objects SQL Context and Spark Context.
	-The SQL Context is used to access the SQL API's
	-The Spark Context is used to access the RDD API's
-Let's execute the 

vaccinationsDF = spark\
                    .read\
                    .format("csv")\
                     .option("header", "true")\
                     .option("inferSchema", "true")\
                    .load("C:/Users/91961/Documents/Rajath/PySpark/Datasets/Datasets/covid/archive/country_vaccinations.csv")
.read > To read the data and we |||ly use write for writing the data
.format > Read the given CSV file
.header,true > Tells to use the column name
.inferschema,true > Tells to use the schema/datatypes as given in the excel or else everything will considered as the string by default.
******Even when the .header,true and .inferschema,true is deleted even then in the UI it tells 1 output row is read, because it will 
read the first line and understand how many columns this csv files has with help of delimiter.
******Even when the .inferschema,true is only deleted and run the step, the metadata time will be 0ms and output row will be 1 because .header,true
**When inferschema is set to false, it takes less time to fetch meta data, becuase everything will be string, it makes it's work easy
.option("quote",'"') >> To tell spark that anything defined inside the '"' as a complete/whole field and don't consider as two different ex: "Hey, I'm Rajath" (it shouldn't split Hey and I'm Rajath, please consider as whole string)

-Date will be considered as string while infering schema from CSV file, if required we need to cast it as timestamp later.

== Physical Plan ==
CollectLimit (3)
+- * Filter (2)
   +- Scan text  (1)

(1) Scan text 
Output [1]: [value#0]
Batched: false
Location: InMemoryFileIndex [file:/C:/Users/91961/Documents/Rajath/PySpark/Datasets/Datasets/covid/archive/country_vaccinations.csv]
ReadSchema: struct<value:string>

(2) Filter [codegen id : 1]
Input [1]: [value#0]
Condition : (length(trim(value#0, None)) > 0)

(3) CollectLimit
Input [1]: [value#0]
Arguments: 1

Applicable for CSV:
------------------
dropmalformed : Remove the wrong format or corrupted records
permissive : Permit everything wherever it fails to parse mark it as the null
failfast : fail immediately if any records goes out of the agreed datatype or found the corrupted record


*****
Check with Prudhvi on a sample template for performing validation of data prior processing : Answer is found in the last section of Spark SQL CSV file processing
If for CSV no header is provided, how to define header/column names: Answer is in the struct type of the schema has column and datatype required mentioned.
*****

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Write to S3/HDFS storage:
-The jar 'org.apache.hadoop:hadoop-aws:3.2.0' is required to write data into s3
-In the local we have one executor and in this example it is running 4 tasks, hence 4 files.
-CRC files are generated when files are written to local file system, but when written to S3 or HDFS, this not generated.
-Let's repartition, the date column will act as predicate, the order by should be used only if the column is part of predicate pushdown.

*******When an action is observed in the spark, what is the spark going to do? what we will be seeing in the UI?
-A job will be created.
-No of jobs is equal to the no of actions
-For each and every action a job is created.
-Next level of job is the stages.
-The no of stages is decided based on the type of transformations.
-If it is narrow transformation it is applied on the Parent RDD.
-If there is a wide transformation, then shuffle RDDs are created.
-What stages consists of ? it consists are tasks.
-No. of tasks depends on the no of partitions.
-At what all stages we can decide no. of partitions?
	While reading the data
	While applying the wide transformation
	While writing the data also
-While writing the data we can use two things.
	repartition > To increase / decrease the partitions
	coalesce > To decrease the partition
-Behind the hood all the DataFrames or each sql query will be linked with a job > stages > RDDs (catalyst optimizer takes care of converting the DF to RDD).
-For show and for loading the data a job will be created.
	
++++++++++++++++++++++++++++++++++++++++

*******The jars will be downloaded to C:\Users\91961\.ivy2\jars, important interview questions.
The above path will be generally configured by Admins, we can come to know about it through Spark UI, under environmanent tab.

========================================================================
-In Spark-Core we can use flatmap to flatten an array and we need to use explode to flatten an array in SQL.
-Generally we see two nested types of Datastructures/Datatypes in SparkSQL, they're struct and an array.
-To flatten an array we can use >>>> explode(col("dataset")).alias('explode_col')
-To flatten a struct we can use >>>> col('explode_col.*')
-We're doing Dataframe explode, what's the purpose of it?
-Can we explode struct column or only an array column?
	-Above question is for Json example, here we are doing it for struct column
-[["city":"rjy",pincode:533101],["city":"hyd",pincode:533102]] >> It's an array of struct.
-Explode of DF works as below,
-Prior to explode:
Employee_ID		Employee_Name	Address_Info
1004			Prudhvi			[["city":"rjy",pincode:533101],["city":"hyd",pincode:533102]]

-Post Explode:
Employee_ID		Employee_Name	Address_Info
1004			Prudhvi			["city":"rjy",pincode:533101]
1004			Prudhvi			["city":"hyd",pincode:533102]

-.withColumn helps in creating a new column.
-##json.loads helps in creating json objects by taking in json strings
-To register anything as udf we need below things:
	-Either lambda function or user defined Python function
	-Return type of the function
	-Say in the below example convert_Str_JSON_UDF_1 is the user defined python function which takes col("spatial") values as an input for each row and going to return json object as an output.

*****How to create an UDF? ##Refer Below
*****Use of an UDF? ##Refer Below
*****Use of explode function?

a	b	c
1	2	3
4	5	6
7	8	9
10	11	12

Need to add a column 'd' by adding a+b

def add(a,b):
	return a+b
##******Below is the way to create an UDF/to register function add as an udf
add_udf = udf(add, DoubleType()) ##DoubleType() is part of SparkSQL datatypes, we can also use IntegerType()
df.withColumn("d",add_udf(col('a'),col('b')))

Output:
=======
a	b	c	d
1	2	3	3.0
4	5	6	9.0
7	8	9	15.0
10	11	12	21.0

Explanation given below:
Row1:a=1,b=2 add(1,2)=>3=d
Row2:a=4,b=5 add(4,5)=>9=d
:
: