Spark-SQL : 25Jan2022
-We still have a Spark-Core which consists of RDD.
-On top of the spark-core Spark-SQL is built.
-Unlike Scala PySpark only supports Dataframes and not the Datasets.
-When anything is submitted to Spark-SQL will get submitted to catalyst optimizer this inturns convert into RDDs, the catalyst takes care of creating logical and physical plan(RDD).
-Spark SQL > DataFrames > Catalyst Optimizer > RDDs > Partitions > Executors.
-Spark Session is an object which internally consists of two more objects SQL Context and Spark Context.
	-The SQL Context is used to access the SQL API's
	-The Spark Context is used to access the RDD API's
-Let's execute the 

vaccinationsDF = spark\
                    .read\
                    .format("csv")\
                     .option("header", "true")\
                     .option("inferSchema", "true")\
                    .load("C:/Users/91961/Documents/Rajath/PySpark/Datasets/Datasets/covid/archive/country_vaccinations.csv")
.read > To read the data and we |||ly use write for writing the data
.format > Read the given CSV file
.header,true > Tells to use the column name
.inferschema,true > Tells to use the schema/datatypes as given in the excel or else everything will considered as the string by default.
******Even when the .header,true and .inferschema,true is deleted even then in the UI it tells 1 output row is read, because it will 
read the first line and understand how many columns this csv files has with help of delimiter.
******Even when the .inferschema,true is only deleted and run the step, the metadata time will be 0ms and output row will be 1 because .header,true
**When inferschema is set to false, it takes less time to fetch meta data, becuase everything will be string, it makes it's work easy
.option("quote",'"') >> To tell spark that anything defined inside the '"' as a complete/whole field and don't consider as two different ex: "Hey, I'm Rajath" (it shouldn't split Hey and I'm Rajath, please consider as whole string)

-Date will be considered as string while infering schema from CSV file, if required we need to cast it as timestamp later.

== Physical Plan ==
CollectLimit (3)
+- * Filter (2)
   +- Scan text  (1)

(1) Scan text 
Output [1]: [value#0]
Batched: false
Location: InMemoryFileIndex [file:/C:/Users/91961/Documents/Rajath/PySpark/Datasets/Datasets/covid/archive/country_vaccinations.csv]
ReadSchema: struct<value:string>

(2) Filter [codegen id : 1]
Input [1]: [value#0]
Condition : (length(trim(value#0, None)) > 0)

(3) CollectLimit
Input [1]: [value#0]
Arguments: 1

Applicable for CSV:
------------------
dropmalformed : Remove the wrong format or corrupted records
permissive : Permit everything wherever it fails to parse mark it as the null
failfast : fail immediately if any records goes out of the agreed datatype or found the corrupted record


*****
Check with Prudhvi on a sample template for performing validation of data prior processing : Answer is found in the last section of Spark SQL CSV file processing
If for CSV no header is provided, how to define header/column names: Answer is in the struct type of the schema has column and datatype required mentioned.
*****

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Write to S3/HDFS storage:
-The jar 'org.apache.hadoop:hadoop-aws:3.2.0' is required to write data into s3
-In the local we have one executor and in this example it is running 4 tasks, hence 4 files.
-CRC files are generated when files are written to local file system, but when written to S3 or HDFS, this not generated.
-Let's repartition, the date column will act as predicate, the order by should be used only if the column is part of predicate pushdown.

*******When an action is observed in the spark, what is the spark going to do? what we will be seeing in the UI?
-A job will be created.
-No of jobs is equal to the no of actions
-For each and every action a job is created.
-Next level of job is the stages.
-The no of stages is decided based on the type of transformations.
-If it is narrow transformation it is applied on the Parent RDD.
-If there is a wide transformation, then shuffle RDDs are created.
-What stages consists of ? it consists are tasks.
-No. of tasks depends on the no of partitions.
-At what all stages we can decide no. of partitions?
	While reading the data
	While applying the wide transformation
	While writing the data also
-While writing the data we can use two things.
	repartition > To increase / decrease the partitions
	coalesce > To decrease the partition
-Behind the hood all the DataFrames or each sql query will be linked with a job > stages > RDDs (catalyst optimizer takes care of converting the DF to RDD).
-For show and for loading the data a job will be created.
	
++++++++++++++++++++++++++++++++++++++++
