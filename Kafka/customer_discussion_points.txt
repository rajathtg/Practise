Hi All,
Good Evening!
Thank you for this opportunity, well myself Rajath, I'm a native to Bangalore city which is one of the fast growing IT hub in India, I'm also a Graduate Engineer with an overall experience of 6.7 years in IT industry, the work experience is spread across healthcare, telecom and public sector domains and also I carry a good hands on experience in tools/applications/languages such as,
	-SQL
	-Python
	-Java
	-Hadoop ecosystem (Data Engineers / Data Lake to be mentioned)
	-Apache Spark
	-Apache Kafka
	-Scheduling tool Apache Airflow
	-Object Storage MinIO (AWS-S3 replica)
	-DB's like Apache Druid, MySQL and Oracle
	-Finally the reporting tools Power BI & Apache Superset

Data Engineer and in a Data Lake
Volume to be mentioned
Service handled half million records / hour and 20+ Nodes
Input users and end users
Telecom domain and Insurance domain experience
Close to 7 years
Designing
Leading the team size of so and so
Architecting the platform
Business of the project, Volumes and Nodes

Kafka, Airflow, Apache Druid, Apache Spark
Power BI to 
platform

Update the details in the DB instead on the incoming Kafka Messages, instead of completing BI

Data Lake side 

Producer API, Consumer API?
Kafka Version
How many Topics your 
400KB 
Message 

5 Minutes what are messages coming in
Overall Pay load per day?
Total record count??

Questions :
-Have an idea on techinal side, we can ask related to business?
-Do we have anything with respect to Analytical side

Coming to my recent assignment/project in TCS : UIDAI (Stands for Unique Identification Authority of India)
======================================================
It is one of the largest agency under the central government of India mandated to collect demographic and biometric information of the country's residents through enrolments and to provide services to residents as part of their identity during any kind of Authentication (OTP/Finger-Print/Iris)

My Roles and Responsibilities:
-----------------------------
-I worked as a Data Engineer in Datalake Team at client location
-My task was to actively participate in the customer discussions and get the reporting requirement
-Post which I've generated the reports by building a Kafka data pipeline to process the data in the near real time
-Later I held a demo on the generated reports and incorporate review comments if any and promote it to production
-The reports generated by me has helped the customers in taking key business decisions

Overview of BI pipeline:
------------------------
In BI pipeline, I've primarily worked on 
	-Kafka (KStreams/KTable/Kafka Connect)
	-Java
	-Druid
	-MinIO
	-Power BI

Coming to Data Flow and Processing,
I start my dataprocessing, once the authentication events of the residents (OTP/FP/Iris) are made available to BI Kafka Source Topics by the auth team,
	-I've worked on developing KStreams for processing and aggregating the data
	-The format of the data used is AVRO and I've created the required schema and serialization for processing
	-Post processing, the data is filtered w.r.t to authentication types and written to multiple target topics
	-I've created scripts to ingest the data available on the target topics into DB Apache Druid, the reason why Apache Druid is because, it has capability to read data in real-time and ingest into the tables
	-I've created a Kafka Sink Connector to write a copy of the data onto Object Storage MinIO as well (replica of AWS-S3) for the archival purpose
	-Apache Druid is integrated with Power BI to generate the reports,
		-I've created reports such as,
			-Successful Authentication based on the type of authentication and the vendor who has utlized it : This report helps business in billing the Vendor for the service provided
			-Drill down charts to VTC level (Village, Town and City)
			-Unified Dashboards of which includes multiple charts to give Authentication trends at one place

Note: I've also worked on enrolment data, the processing of it is also built on the same pipline

-------------------------------------------------------------------------------------------------------------------------------------------------------------

Second assignment/project is also with the UIDAI client:
========================================================

Moto : As part of covid pandemic, UIDAI started to work on a new project called as FaceRD (Face Recognition Device), to provide/encourage a contactless authentication for the residents, as part of this ML engineers were assigned to train and test their models on Face Liveliness

My Roles and Responsibilities:
------------------------------
-To actively participate in the data requirement discussions with ML team engineers
-To develop the code using the PySpark Datalake pipeline to meet ML engineers requirement
-To validate the processed data with the ML engineers in the lower environments and accordingly incorporate any additional requirements
-Also to develop better business insights on the processed data using SQL for the end users

Batch Pipeline Overview:
------------------------
In BI Batch pipeline, I've primarily worked on,
	-Spark
	-Python
	-MySQL
	-MinIO
	-Airflow
	-Oracle DB

Coming to data flow and processing,
Once the data is made available on the RDBMS DB by the FaceRD field engineers,
	-I've developed a PySpark code to establish a JDBC connection with MySQL tables
	-11 Nodes, multiple TBs of data i.e. 1 to 1.5 TB data / per day
	-The huge FaceRD datasets are extracted, initial level of data profiling/cleansing is done
	-A Joining is performed at Spark level across the multiple Data Frames created for FaceRD and the required Filter/transformation/aggregations are also applied to meet the requirement of ML engineers
	-Joining is performed for enrichment of data
	-The end result is written on to a object storage MinIO (AWS-S3 replica) and also to Oracle DB (because othe teams also wanted to use it)
	-Data size is high Spark optimisation is implemented to avoid high load issue
	-The entire process of ETL is scheduled with the help of the Airflow
	-ML engineers extract the processed data from the S3 and train their model on the Face Image Liveliness (a certain threshold will be set, the image captured need to meet the threshold or else it will be considered as Fake)


I also worked on analytical part as well to generate business insights
-I've also created Stored procedures and Views on the data stored in Oracle DB to enrich the data for business insights

I also got exposure to Power BI to work on real-time reporting
	

This was all about my recent project and my experience thanks for your time, may I know you've any questions

RDBMS > Spark > Data Lake > Object Storage > ML Engineer > 
		
		
=====================================================================================================

Python operator
Bash operator
How to create the operator?
Kafka Leader and Follower

Simple example for creating Stored Procedures what logic is incorporated (Create table / joins)

Dockerised way of running the commands

Connaught palace market