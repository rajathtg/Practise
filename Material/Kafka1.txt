Kafka:
=======
Flume > Faster Stream Capturing(But no 100% delivery guarantee), suitable for social media and not for sensitive application.
Kafka > -Faster Stream Capturing + Messaging(Broker system/middle ware/connecting) and 100% delivery guarantee.
	-This is large distributed system like Hadoop, hence high loads of data can be tackled by this.
	-This has topics and it maintains queues of messages if target application is busy and depending on availability 
	 it will despatch the data(similar to messages loading in WhatsApp when net is turned on).
	-Topics(which are nothing but queues) are further divided into partitions(will have replicas similar to blocks in 
	 Hadoop to maintain high availabilty) and partitions(including replicas) into different nodes(even here they follow rule like no two same replica in same node).
Storm > This can perform Live Analytics over each event (i.e. the message generated by source / captured by the middle ware system).
Spark Streaming > The purpose is micro-batching(if we're interested to list how many bad words / good words are tweeted like that).

Kafka:
-Main advantage on why to use this is:
	100% Message Delivery guarantee.
	Large volumes of travsactional support(Reading and Writing).
	Persistance of mesaages(even available on JMS,websphere,tibco etc), even we can decide for many days this message needs to be persisted into the disk.
	It supports all type of applicatiosn(built on Java,Python,CSharp), even can stream from RDBMS,file sources, hadoop kind of streams, NoSQL.
	This have different API using which can be integrated with IOT devices.

Kafka Components:
-ZooKeeper > Without this there is no kafka cluster(There are 3 types of Kafkacluster(will see this later))
	     Generally it is a co-ordinator system which maintains cluster state.
	     State in essence the state of nodes(Brokers) which is up, which one is dead etc
	     Broker to broker metadata is exchanged via ZooKeeper.
	     ZooKeeper plays two responsibilities:
			Cluster state.
			Exchanges information between brokers
	     Consider there are 4 brokers B1,B2,B3 & B4 all the 4 should be connected to a ZooKeeper port 8082 and this whole thing of brokers will be
	     taken as one single cluster.
	     In the above listed 4 brokers if 2 clusters (B1 & B2) are connected to 8082 of ZK and remaning broker(B3 & B4) are connected to ZK 8081 then
	     we can take  there are two different Kafka clusters.
-Broker(Nodes) > In general we can call this as a server, every broker is acting as a slave and at the same time every broker is acting as a Master for some Topic.
		 There is no exact slave and master, depending for each topic, master and slave is formed.
		 Ex: For topic 1 B2,B3 & B4 act as slave and B1 as Master. For topic2 B3 could be Master and remaining as slave.
		 *****Broker simply maintains Topics and Messages, here topics are message holding units.
		      Also saves messages to topics and serves messages to applications. 
		 How we tables in RDBMS, queues in JMS similarly Topics in Brokers.
		 Because of Brokers in Kafka clusters we can have multiple masters and multiple slaves.
		 Due to multiple master and slave architecture very well load balancing can be achieved.
-Topic > As mentioned it's message holding unit in Kafka.
	 Record means it's data line but message means we have some headers and headers will be of the form key(can be null,offset or custom value) and value(here value is dataline).
	 Ex: Topic1 > 2 partitions > Each partition has 3 replica > 6 partition > Loaded/distributed into different Nodes / Brokers (no 2 same replica into single node)
	 ****It's not necessary for each topic to have 2 partition, depending on req for other topics it can vary from 3 to more partitions.
	 As mentioned one partition will have 3 replicas and among 3 one will act as master and remaining as lead. If one of replica is down then dynamically one more is created to achieve 3 replica concept.
	 The number of partitions formed depends on work load
	 The number of replicas can be controlled.
-Producer > Responsible to write messages into topics.
-Consumer > responsible to read messages from topics.
==================================================================================================================================================

Kafka Setup:
------------

Spark: Spark streaming and Kafka Integration steps:
>Start zookeeper server.
>Start Kafka brokers [one or more].
>Create topic.
>Start console producer.
>Start console consumer.
>Create spark streaming context, which streams from kafka topic.
>Perform transformations or aggregations
>Output operation : Which will direct the

Kafka commands:
--------------
bin/zookeeper-server-start.sh config/zookeeper.properties

bin/kafka-server-start.sh config/server.properties

bin/kafka-topics.sh --list --zookeeper localhost:2181

bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test

bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning

bin/kafka-topics.sh --create --zookeeper localhost:2182 --replication-factor 1 --partitions 1 --topic test

[cloudera@quickstart ~]$ su root
Password:
Note: password is as usual Cloudera

[root@quickstart cloudera]# cd kafka_2.11-0.10.2.1

[root@quickstart kafka_2.11-0.10.2.1]# ls
bin  config  libs  LICENSE  logs  NOTICE  site-docs

Next we need to give zookeper property file, within it the host name is local host and they will 
assign the port the number, with what port number the zookeeper started all broker should refer
the same port number.
If for each broker if we give zookeeper separate port number then they're separate Kafka clusters.

To start the Zookeeper:

[root@quickstart kafka_2.11-0.10.2.1]# bin/zookeeper-server-start.sh config/zookeeper.properties

Once we execute the above command lot things keep running at the last when we get the message that NIOServerCnxnFactory then it means server is successfully started.
[2020-02-28 15:39:24,912] INFO binding to port 0.0.0.0/0.0.0.0:2182 (org.apache.zookeeper.server.NIOServerCnxnFactory)

Definitely the start up of zookeeper file will be failing because the port kept into properties will already be used by 
someother service.
To check the engaged port details, use the below command:
[root@quickstart kafka_2.11-0.10.2.1]# jps
3090 JournalNode
2980 DataNode
3524 NodeManager
3732 JobHistoryServer
3205 NameNode
3305 SecondaryNameNode
3625 ResourceManager
6219 QuorumPeerMain
6524 Jps
3453 Bootstrap
We can either kill them and restart the zookepeer or make changes in the zookeeper config file.
Let's check the content of the file using below command:
[root@quickstart kafka_2.11-0.10.2.1]# vi config/zookeeper.properties

>Next compnent is server, server means broker:
First let's do single broker cluster, when we have single broker we have two problems:
	-Partition distribution will not happen even though your topic is having 10 partition and all 10 to be maintained by 1 and heavy load will be applied.
	Note: If partitions are distributed load will be effectively balanced, solution to above problem.
	-If there is single node broker or single node Kafka cluster, fault tolerance can't be provided.
	-I've a partition and but I've only one broker then I can't go with more than one replica.
	-It's good to have two, just for practise we work with single node or broker.

The broker details are kept into server.properties that is > bin/kafka-server-start.sh config/server.properties
-Let's look into the server.properties and understand the important things that needs to be changed.

[root@quickstart kafka_2.11-0.10.2.1]# ls
bin  config  libs  LICENSE  logs  NOTICE  site-docs
[root@quickstart kafka_2.11-0.10.2.1]# ls config
connect-console-sink.properties    consumer.properties
connect-console-source.properties  log4j.properties
connect-distributed.properties     producer.properties
connect-file-sink.properties       server.properties
connect-file-source.properties     tools-log4j.properties
connect-log4j.properties           zookeeper.properties
connect-standalone.properties
[root@quickstart kafka_2.11-0.10.2.1]# cat config/server.properties
-At the begining we have something called as id, by default it is 0, if we're starting one more broker id=1
-We also have host name for this consider it is 9092, next broker it should be 9093.
-By default we have /tmp/kafka-logs as directory for the next broker do change it, same directory shouldn't be used. 
-Check whether the port number matches with the one mentioned in the Zookeeper config file, if yes go ahead or else
 All the brokers should use the same Zookeeper host.
make the required changes.

[root@quickstart kafka_2.11-0.10.2.1]# bin/kafka-server-start.sh config/server.properties

[2020-02-28 17:16:02,812] INFO New leader is 0 (kafka.server.ZookeeperLeaderElector$LeaderChangeListener)
If above result is been present then it's working fine.
Now broker is ready.

-let's work on our message holding unit, topic:
Now there is no necessity to work under root.

[cloudera@quickstart ~]$ cd kafka_2.11-0.10.2.1
[cloudera@quickstart kafka_2.11-0.10.2.1]$ bin/kafka-topics.sh --create --zookeeper localhost:2182 --replication-factor 1 --partitions 1 --topic mytopic
Created topic "mytopic".

Let's create one more topic with 2 partitions

create --zookeeper localhost:2182 --replication-factor 1 --partitions 2 --topic urtopic
Created topic "urtopic".

If we see the above command the topic uses correct host of zookeeper
Topic is logical and partition is physical default is 1 if needed we update.
In single broker we can keep multiple topics and even multiple partitions but not multiple replicas.
Note: No. of replicas should be less than or equal to number of brokers.

[cloudera@quickstart kafka_2.11-0.10.2.1]$ bin/kafka-topics.sh --list --zookeeper localhost:2182
mytopic
urtopic 

[cloudera@quickstart kafka_2.11-0.10.2.1]$ bin/kafka-topics.sh --describe --zookeeper localhost:2182
Topic:mytopic	PartitionCount:1	ReplicationFactor:1	Configs:
	Topic: mytopic	Partition: 0	Leader: 0	Replicas: 0	Isr: 0
Topic:urtopic	PartitionCount:2	ReplicationFactor:1	Configs:
	Topic: urtopic	Partition: 0	Leader: 0	Replicas: 0	Isr: 0
	Topic: urtopic	Partition: 1	Leader: 0	Replicas: 0	Isr: 0

-Now the topics are ready, next task is to write messages into topics using Producer....
-For testing purpose of cluster Kafka provides console Producer using which we can write test messages into topic, similarly to check 
 whether streaming is happening or not consumer is responsible to test it, this console is also provided Kafka.
-Only draw back with these both is we cannot do customisation, in built API's are taing care of on both producer and consumer end.

-Let's start console producer:
[cloudera@quickstart kafka_2.11-0.10.2.1]$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic mytopic

-If we have multiple brokers then we can mention it with commas localhost:9092,localhost:9091,localhost:9090
-Let's write the data into it, once we write it will be kept at topic.
-In console whenever we press enter one event is created.
-Difference between message and record is
 Message has some shape like key and value(enter event record is a value).
 By default key is null.
 If we want we can apply own keys.
 To divert the few things into particular partitions then we need to go with own keys.
 Similar to MapReduce we can write custom partitions over here as well.

[cloudera@quickstart kafka_2.11-0.10.2.1]$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic mytopic
Hello
How are you
Bye

Note: Just type enter after bye then open new console for consumer.

-Next is consumer:

bin/kafka-console-consumer.sh --bootstrap-server localhost:2182 --topic mytopic --from-beginning

-We specify zookeeper here because consumer doesn't know in which broker topic is available, ofcourse zookeeper also doesn't 
 know it will also contact so and so broker(meta data).
-We need to mention from beginning or else only new events alone will be captured.

[cloudera@quickstart kafka_2.11-0.10.2.1]$ bin/kafka-console-consumer.sh --bootstrap-server localhost:2182 --topic mytopic --from-beginning
Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new
consumer by passing [bootstrap-server] instead of [zookeeper].
hello
how are you
bye

-These messages availability can be planned, for years long we can't plan, but to some extent we can.
 Because if target application is sleeping what will you do, it got wait and deliver once it is awake.???

Later whatever we write into the producer it will be captured by the consumer just like that.

[cloudera@quickstart kafka_2.11-0.10.2.1]$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic mytopic
Hello
How are you
Bye
O2

[cloudera@quickstart kafka_2.11-0.10.2.1]$ bin/kafka-console-consumer.sh --bootstrap-server localhost:2182 --topic mytopic --from-beginning
Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new
consumer by passing [bootstrap-server] instead of [zookeeper].
hello
how are you
bye
O2

-Consider you stopped Consumer, producer keeps running as there is no connection between the two.
-When we restart the consumer and mention from beginning, everything will be still on board.
-Let's kill producer, consumer is still running and waiting for any data.
-Let's kill the broker, producer and consumer both are running but know mediator for messages between producer and consumer.
-There is no fault tolerance in this cluster setup, henec we need start kafka cluster with multiple brokers.
-It's good keep kafka as separate cluster and integrate it. Instead of having it with hadoop or spark.
-Again it depends if majority of interaction is with hadoop then keep it as part of hadoop or any application for that matter.

===================================================================================================================================
-Let's work on fault tolerance and overcome that issue:
-First let's kill all running applications using ^C(Control Copy)
-Start Zookeeper again using server-start.sh............
-Next start the server number 1, i.e broker number 1
-Next we need to start broker number 2, before that we got to make some changes in properties(let's not disturb the existing file
 instead let's take a copy of the file and edit that).

[cloudera@quickstart kafka_2.11-0.10.2.1]$ cp config/server.properties config/server2.properties
[cloudera@quickstart kafka_2.11-0.10.2.1]$ ls
bin  config  libs  LICENSE  logs  NOTICE  site-docs
[cloudera@quickstart kafka_2.11-0.10.2.1]$ ls config
connect-console-sink.properties    consumer.properties
connect-console-source.properties  log4j.properties
connect-distributed.properties     producer.properties
connect-file-sink.properties       server2.properties
connect-file-source.properties     server.properties
connect-log4j.properties           tools-log4j.properties
connect-standalone.properties      zookeeper.properties

-Where do the changes required??
	-Broker Id, change id = 1
	-Port details,
		FORMAT:
		#     listeners = listener_name://host_name:port
		#   EXAMPLE:
		#     listeners = PLAINTEXT://your.host.name:9092
		#listeners=PLAINTEXT://:9092

		FORMAT:
		#     listeners = listener_name://loca:port
		#   EXAMPLE:
		#     listeners = PLAINTEXT://localhost:9093
		listeners=PLAINTEXT://:9093
	-Logs
		# A comma seperated list of directories under which to store log files
		log.dirs=/tmp/kafka-logs

		# A comma seperated list of directories under which to store log files
		log.dirs=/tmp/kafka-logs2
	-Zookeeper port should be the same
	-Save and close
-Let's start the new server.
cloudera@quickstart kafka_2.11-0.10.2.1]$ bin/kafka-server-start.sh config/server2.properties
We get below confirmation as kafka server 2:
[2020-02-29 07:31:46,156] INFO [Kafka Server 1], started (kafka.server.KafkaServer)

-Let's create few more topics
-we can increase the replicas

[cloudera@quickstart kafka_2.11-0.10.2.1]$ bin/kafktopics.sh --create --zookeeper localhost:2182 --replication-factor 2 --partitions 1 --topic ourtopic
Created topic "ourtopic".

[cloudera@quickstart kafka_2.11-0.10.2.1]$ bin/kafka-topics.sh --list --zookeeper localhost:2182
mytopic
ourtopic
urtopic

-Here zookeeper plays major role of collecting, interacting....with brokers with the help of metadata present with brokers (****metadata will not be with zookeeper).

-let's start console producer: We have two servers now.
-Here our topic we're using which has 1 partition with 2 replicas.
-If we've 2 brokers and along with topic of 2 replicas then these 2 replicas are distributed into 2 brokers.
-Advantage: Now producer will write into lead replica only(interaction is always with this) and later broker will write into the backup replicas
-If lead replica is in in broker 1, if broker 1 is down then replica in other broker will become lead replica and business continues.

[cloudera@quickstart kafka_2.11-0.10.2.1]$ bin/kafka-console-producer.sh --broker-list localhost:9092,localhost:9093 --topic ourtopic
one
two
three
four five

-Above thinks are stored into partition that to lead replica, later replica copies made by broker and communication is taken care by main boss
 zookeeper.

-Let's start console consumer:
[cloudera@quickstart kafka_2.11-0.10.2.1]$ bin/kafka-console-conmer.sh --bootstrap-server localhost:2182 --topic ourtopic --from-beginning
Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new
consumer by passing [bootstrap-server] instead of [zookeeper].
one
two
three
four five

-Consumer as no idea in which partition what is there or even about replica, everything is taken care by Zookeeper.
-Let's kill broker 1, still business continues with broker 2 and it's topic takes care of lead replica and other stuff.
-If after 10 minutes broker 1 is online, even then lead replica will be broker 2 it will not get transferred.
-We discussed that if there are 2 replicas one has to be leader who will decide which replica in which broker will be leader
 it is done throiugh voting, we're yet to discuss this.
-This is actually single node multi broker system.
-If the node is down everything is down hence we need a multi node cluster.
-In kafka there is no master slave architecture, everyone os slave and everyone is master at the same time.

*****Some Theory:
-Consider we have 3 brokers with respective topics and partitions:

b1 t1(p1-r1)-leadreplica (through voting) t1(p2-r3)
b2 t1(p1-r2)				  t1(p2-r2)
b3 t1(p1-r3)				  t1(p2-r1)-lr (voting)

-If we notice b3 is leader for p2, but slave for partition1
-similarly b1 is leader for p1 and slave for p2
-It's like I'm boss when in my project, but slave when working for others
-This actually helps when there are 1000 node cluster, 1 single has to maintain all these means
 it's difficult hence this concept will help and in real time 1 master will interact max to max with 10 nodes only
-FB and LinkedIn enjoy trillions trillions of transcation in real time.
-This concept is used in IMPS and Neft as well
=====================================================================================================================

